{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUSTOM ATTRIBUTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matcher\n",
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy.tokens import Token, Span\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "from py2neo import Graph,Subgraph,Node,Relationship,cypher,data\n",
    "from pandas import DataFrame\n",
    "\n",
    "graph = Graph(\"bolt://localhost:7687\", user=\"neo4j\", password=\"graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matcher Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that        SCONJ     det       \n",
      "smart       ADJ       amod      \n",
      "kid         NOUN      nsubj     \n",
      "is          AUX       aux       \n",
      "using       VERB      ROOT      \n",
      "a           DET       det       \n",
      "kindle      NOUN      dobj      \n",
      "\n",
      "\n",
      "Total matches found: 3\n",
      "Match found: smart kid\n",
      "Match found: kid\n",
      "Match found: kindle\n"
     ]
    }
   ],
   "source": [
    "# Load model, create NLP object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# create doc object\n",
    "doc = nlp(\"that smart kid is using a kindle\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<12}{token.pos_:<10}{token.dep_:<10}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# identify a pattern\n",
    "pattern = [{\"POS\": \"ADJ\",\"OP\":\"?\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"ADJ_NOUN_PATTERN\", None, pattern)\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Attribute Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I           PRON      nsubj     0         \n",
      "have        AUX       ROOT      0         \n",
      "an          DET       det       0         \n",
      "apple       NOUN      dobj      1         \n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "fruit_getter = lambda token: token.text in (\"apple\", \"pear\", \"banana\")\n",
    "Token.set_extension(\"is_fruit\", getter=fruit_getter,force=True)\n",
    "doc = nlp(\"I have an apple\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<12}{token.pos_:<10}{token.dep_:<10}{token._.is_fruit:<10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Custom Attribute: okg_class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph class Lookup: getter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def okg_class_lookup(token):\n",
    "    name = str.capitalize(token.text)\n",
    "    cursor = graph.run(\"MATCH (token:Token {name:'\"+name+\"'})-[r:INSTANCE_OF]->(c:Class) RETURN c.name\")\n",
    "    df = DataFrame(cursor)\n",
    "    if(df.empty):\n",
    "        return (\"not found\")\n",
    "    else:\n",
    "        classe= str(df.iloc[0,0])\n",
    "        return(classe)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### okg_class test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I           PRON      nsubj     not found\n",
      "have        AUX       ccomp     not found\n",
      "a           DET       det       not found\n",
      "missile     NOUN      dobj      Technology\n",
      ",           PUNCT     punct     not found\n",
      "we          PRON      nsubj     not found\n",
      "have        AUX       ROOT      not found\n",
      "a           DET       det       not found\n",
      "warfare     NOUN      dobj      Application\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "okg_class = lambda token: okg_class_lookup(token)\n",
    "Token.set_extension(\"okg_class\", getter= okg_class,force=True)\n",
    "doc = nlp(\"I have a missile, we have a warfare\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<12}{token.pos_:<10}{token.dep_:<10}{token._.okg_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhraseMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhraseMatcher usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newLine():\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBAMA Barack Obama\n"
     ]
    }
   ],
   "source": [
    "# from spacy docs\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# load nlp object\n",
    "doc = nlp(\"Barack Obama lifts America one last time in emotional farewell\")\n",
    "\n",
    "# load matcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# add entry to matcher\n",
    "matcher.add(\"OBAMA\", None, nlp(\"Barack Obama\"))\n",
    "\n",
    "# find matches in the document\n",
    "matches = matcher(doc)\n",
    "       \n",
    "for match_id, start, end in matches:\n",
    "    rule_id = nlp.vocab.strings[match_id]  # get the unicode ID, i.e. 'COLOR'\n",
    "    span = doc[start : end]  # get the matched slice of the doc\n",
    "    print(rule_id, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nicola's Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e74a97e1a8d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhraseMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'terms.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "nlp = load_model()\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "with open('terms.txt', 'r') as f:\n",
    "    terms = f.readlines()\n",
    "    terms = [t.replace('\\n','') for t in terms if t != '\\n']\n",
    "\n",
    "def add_phraseMatcher_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label = \"SOFT SKILL\")\n",
    "    try: \n",
    "        doc.ents += (entity,)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", add_phraseMatcher_ent, *patterns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matcher\n",
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy.tokens import Token, Span\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "from py2neo import Graph,Subgraph,Node,Relationship,cypher,data\n",
    "from pandas import DataFrame\n",
    "\n",
    "graph = Graph(\"bolt://localhost:7687\", user=\"neo4j\", password=\"graph\")\n",
    "\n",
    "# Load model, create NLP object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getter function\n",
    "def add_phraseMatcher_ent(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label = \"SOFT SKILL\") # da cambiare\n",
    "    try: \n",
    "        doc.ents += (entity,)\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support data structures\n",
    "cursor = graph.run(\"match (class:Class) return class.name \")\n",
    "df = DataFrame(cursor)\n",
    "classList = list(df[0])\n",
    "\n",
    "# iterate through classes\n",
    "for tokenClass in classList:\n",
    "    cursor = graph.run(\"match (t:Token)-[:INSTANCE_OF]->(c:Class {name:'\"+tokenClass+\"'}) return t.name\")\n",
    "    df = DataFrame(cursor)\n",
    "    tokenList = list(df[0])\n",
    "    \n",
    "    # add matcher rule for each token in the class\n",
    "    patterns = [nlp.make_doc(text) for text in tokenList]\n",
    "    matcher.add(tokenClass, add_phraseMatcher_ent, *patterns)\n",
    "    \n",
    "    # add lowerCase\n",
    "    patterns = [nlp.make_doc(str.lower(text)) for text in tokenList]\n",
    "    matcher.add(tokenClass, add_phraseMatcher_ent, *patterns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missile                          Technology\n",
      "warfare                          Application\n",
      "allocation of decision rights    Approach  \n"
     ]
    }
   ],
   "source": [
    "# load nlp object & matcher\n",
    "doc = nlp(\"I have a Missile, there is a warfare in New York, last night I heard about allocation of decision rights\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    rule_id = nlp.vocab.strings[match_id]\n",
    "    span = doc[start : end]\n",
    "    print(f\"{span.text:<33}{rule_id:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
